{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-01-11T06:14:08.739146Z",
     "iopub.status.busy": "2022-01-11T06:14:08.738762Z",
     "iopub.status.idle": "2022-01-11T06:14:09.581716Z",
     "shell.execute_reply": "2022-01-11T06:14:09.580943Z",
     "shell.execute_reply.started": "2022-01-11T06:14:08.739109Z"
    },
    "id": "Vuw-gNvjqcYe",
    "outputId": "80f57011-5b50-48c5-e299-41a2fdf0f5e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhdbdJOsrbxL"
   },
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYueKGdIHpho"
   },
   "source": [
    "## Loading the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpG2DxEqHFD-"
   },
   "source": [
    "### Custom dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-11T06:14:11.457168Z",
     "iopub.status.busy": "2022-01-11T06:14:11.456681Z",
     "iopub.status.idle": "2022-01-11T06:14:11.469393Z",
     "shell.execute_reply": "2022-01-11T06:14:11.468470Z",
     "shell.execute_reply.started": "2022-01-11T06:14:11.457125Z"
    },
    "id": "dwUE1E83_Iw8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class MTLData(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, mode='train', transform=None):\n",
    "        self.mode = mode # 'train', 'val' or 'test'\n",
    "        self.data_list = []\n",
    "        self.race_labels = [] \n",
    "        self.gender_labels = []\n",
    "        self.age_labels = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        with open(csv_file, newline='') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                self.data_list.append(row['img_name'])\n",
    "                if mode != 'test':\n",
    "                    self.race_labels.append(row['race'])\n",
    "                    self.gender_labels.append(row['gender'])\n",
    "                    self.age_labels.append(row['age'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        max_age = 116\n",
    "        data = Image.open(self.data_list[index])\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        if self.mode == 'test':\n",
    "            return data\n",
    "        race_label = torch.tensor(int(self.race_labels[index]))\n",
    "        gender_label = torch.tensor(int(self.gender_labels[index]))\n",
    "        age_label = torch.tensor(int(self.age_labels[index])/max_age)\n",
    "\n",
    "        return data, race_label, gender_label, age_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tQGLuVWnA-b"
   },
   "source": [
    "### Data augmentation \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-11T06:14:35.040518Z",
     "iopub.status.busy": "2022-01-11T06:14:35.040030Z",
     "iopub.status.idle": "2022-01-11T06:14:35.050897Z",
     "shell.execute_reply": "2022-01-11T06:14:35.049798Z",
     "shell.execute_reply.started": "2022-01-11T06:14:35.040460Z"
    },
    "id": "UIv1VyHXVNTo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms, models\n",
    "# For TRAIN\n",
    "\n",
    "transforms_train = transforms.Compose(\n",
    "    [transforms.Resize((196, 196)),\n",
    "     transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.RandomVerticalFlip(p=0.5),\n",
    "     #transforms.RandomCrop(224, padding=4),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# For VAL, TEST\n",
    "\n",
    "transforms_test = transforms.Compose(\n",
    "    [transforms.Resize((196, 196)),\n",
    "#    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rYptn_YJlFX"
   },
   "source": [
    "### Instantiate dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-11T06:14:38.002771Z",
     "iopub.status.busy": "2022-01-11T06:14:38.002353Z",
     "iopub.status.idle": "2022-01-11T06:14:38.093789Z",
     "shell.execute_reply": "2022-01-11T06:14:38.092623Z",
     "shell.execute_reply.started": "2022-01-11T06:14:38.002718Z"
    },
    "id": "LKjsKyHhZZc6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = MTLData('/home/ctku/Code/Course/DL/Final_Project/train.csv', mode='train', transform=transforms_train)\n",
    "dataset_val = MTLData('/home/ctku/Code/Course/DL/Final_Project/val.csv', mode='val', transform=transforms_test)\n",
    "dataset_test = MTLData('/home/ctku/Code/Course/DL/Final_Project/test.csv', mode='test', transform=transforms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-01-11T06:14:39.110441Z",
     "iopub.status.busy": "2022-01-11T06:14:39.110039Z",
     "iopub.status.idle": "2022-01-11T06:14:39.133523Z",
     "shell.execute_reply": "2022-01-11T06:14:39.132450Z",
     "shell.execute_reply.started": "2022-01-11T06:14:39.110388Z"
    },
    "id": "ZAeQQEEISCJJ",
    "outputId": "7b67a81d-cddd-4e70-bb9c-8a0dcad03272",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first image's shape in dataset_train : torch.Size([3, 196, 196])\n",
      "There are 14223 images in dataset_train.\n"
     ]
    }
   ],
   "source": [
    "print(\"The first image's shape in dataset_train :\", dataset_train.__getitem__(0)[0].size())\n",
    "print(\"There are\", dataset_train.__len__(), \"images in dataset_train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-01-11T06:14:42.246471Z",
     "iopub.status.busy": "2022-01-11T06:14:42.246156Z",
     "iopub.status.idle": "2022-01-11T06:14:42.268381Z",
     "shell.execute_reply": "2022-01-11T06:14:42.267564Z",
     "shell.execute_reply.started": "2022-01-11T06:14:42.246429Z"
    },
    "id": "3GB90onTrArW",
    "outputId": "97183617-7cea-491c-f948-2cdb170963c3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train.__getitem__(10)[1])\n",
    "print(dataset_train.__getitem__(10)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vORA6qkfIj1U"
   },
   "source": [
    "### `DataLoader`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-11T06:14:45.319912Z",
     "iopub.status.busy": "2022-01-11T06:14:45.319498Z",
     "iopub.status.idle": "2022-01-11T06:14:45.329265Z",
     "shell.execute_reply": "2022-01-11T06:14:45.328104Z",
     "shell.execute_reply.started": "2022-01-11T06:14:45.319859Z"
    },
    "id": "RmgA5nYT3XQZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87KYcWknS95z"
   },
   "source": [
    "# Implement MAE using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OH_4NKB9dsZ3"
   },
   "source": [
    "### Define a Masked Autoencoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-11T06:14:47.252996Z",
     "iopub.status.busy": "2022-01-11T06:14:47.252593Z",
     "iopub.status.idle": "2022-01-11T06:14:47.284650Z",
     "shell.execute_reply": "2022-01-11T06:14:47.283359Z",
     "shell.execute_reply.started": "2022-01-11T06:14:47.252944Z"
    },
    "id": "S_HxxEfdO2Ss",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "import math\n",
    "\n",
    "class MAE(nn.Module):\n",
    "    def __init__(self, image_channel, image_size, patch_size, enc_dim, dec_dim, encoder, decoder, mask_ratio=0.75) -> None:\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_dim = patch_size * patch_size * image_channel\n",
    "        self.token_num = (image_size//patch_size)**2\n",
    "        # Note that the input to the torch.nn.Transformer have the batch dimension in the middle: [T(token), B(batch), D(feature)]\n",
    "        \n",
    "        self.shuffler = PatchShuffler(mask_ratio, self.token_num)\n",
    "        \n",
    "        self.register_buffer('enc_pos', positional_encoding(enc_dim, max_len=self.token_num))\n",
    "        self.register_buffer('dec_pos', positional_encoding(dec_dim, max_len=self.token_num))\n",
    "        \n",
    "        self.mask_emb = nn.Parameter(torch.randn(dec_dim))\n",
    "        \n",
    "        self.in_proj = nn.Linear(self.patch_dim, enc_dim)\n",
    "        self.encoder = Transformer(d_model=enc_dim, **encoder)\n",
    "        self.mid_proj = nn.Linear(enc_dim, dec_dim) if enc_dim != dec_dim else nn.Identity()\n",
    "        self.decoder = Transformer(d_model=dec_dim, **decoder)\n",
    "        self.out_proj = nn.Linear(dec_dim, self.patch_dim) if dec_dim != self.patch_dim else nn.Identity()\n",
    "        \n",
    "    def forward(self, img, viz=False):\n",
    "        \n",
    "        self.shuffler.init_rand_idx(img.shape[0], img.device)\n",
    "        \n",
    "        \n",
    "        patches = rearrange(img, 'b c (h s1) (w s2) -> (h w) b (s1 s2 c)', s1=self.patch_size, s2=self.patch_size)\n",
    "        \n",
    "        emb = self.in_proj(patches)\n",
    "\n",
    "        _, enc_inp = self.shuffler.shuffle_split(emb + self.enc_pos)\n",
    "        \n",
    "        \n",
    "        x = self.encoder(enc_inp)\n",
    "        x = self.mid_proj(x)\n",
    "\n",
    "        x = torch.cat([x, self.mask_emb.expand(self.token_num-x.shape[0], x.shape[1], -1)])\n",
    "        dec_pos = self.shuffler.shuffle(self.dec_pos.expand_as(x))\n",
    "        dec_out = self.decoder(x + dec_pos)\n",
    "\n",
    "        pixel_recon = self.out_proj(dec_out)\n",
    "        \n",
    "        inpainted_patches, _ = self.shuffler.split(pixel_recon)\n",
    "        \n",
    "        # get target from input patches\n",
    "        masked_patches, _ = self.shuffler.shuffle_split(patches)\n",
    "        \n",
    "        loss = F.mse_loss(inpainted_patches, masked_patches)\n",
    "        \n",
    "        if viz:\n",
    "            img_recon = self.shuffler.unshuffle(pixel_recon)\n",
    "            img_recon = rearrange('(h w) b (s1 s2 c) -> b c (h s1) (w s2)', h=img.shape[2]//self.patch_size, s1=self.patch_size, s2=self.patch_size)\n",
    "            return {'loss':loss, 'recon': img_recon}\n",
    "        \n",
    "        return {'loss':loss}\n",
    "    \n",
    "class PatchShuffler(nn.Module):\n",
    "    def __init__(self, ratio=0.75, token_num=196):\n",
    "        super().__init__()\n",
    "        self.mask_n = int(ratio*token_num)\n",
    "        self.token_n = token_num\n",
    "        \n",
    "    def init_rand_idx(self, batch_size, device) -> None:\n",
    "        self.rand_idx = torch.rand(self.token_n, batch_size, device=device).argsort(dim=0)\n",
    "        self.sort_idx = torch.argsort(self.rand_idx, dim=0).to(device)\n",
    "        \n",
    "    def shuffle(self, x):\n",
    "        return x.gather(0, self.rand_idx.unsqueeze(-1).expand_as(x))\n",
    "    \n",
    "    def unshuffle(self, x):\n",
    "        return x.gather(0, self.sort_idx.unsqueeze(-1).expand_as(x))\n",
    "    \n",
    "    def shuffle_split(self, x):\n",
    "        x = self.shuffle(x)\n",
    "        return x.split(self.mask_n)\n",
    "\n",
    "    def split(self, x):\n",
    "        return x.split(self.mask_n)\n",
    "    \n",
    "def positional_encoding(d_model, max_len=5000):\n",
    "    position = torch.arange(max_len).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "    pe = torch.zeros(max_len, 1, d_model)\n",
    "    pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, norm, d_model, **layer_kwargs):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, **layer_kwargs),\n",
    "            num_layers=num_layers,\n",
    "            norm=norm\n",
    "        )\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.transformer(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-11T06:14:48.868083Z",
     "iopub.status.busy": "2022-01-11T06:14:48.867672Z",
     "iopub.status.idle": "2022-01-11T06:14:48.881629Z",
     "shell.execute_reply": "2022-01-11T06:14:48.880569Z",
     "shell.execute_reply.started": "2022-01-11T06:14:48.868030Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MTL(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(MTL, self).__init__()\n",
    "        \n",
    "#        ************************************\n",
    "        \n",
    "#        此段將 MAE output 串接MTL的 code \n",
    "        \n",
    "#        ************************************\n",
    "        self.rn = models.resnet34(pretrained=True)\n",
    "        num_ftrs = self.rn.fc.in_features\n",
    "        \n",
    "        self.rn.fc = nn.Linear(num_ftrs, num_ftrs)\n",
    "        self.rn.fc1 = nn.Linear(num_ftrs, 5)\n",
    "        self.rn.fc2 = nn.Linear(num_ftrs, 2)\n",
    "        self.rn.fc3 = nn.Linear(num_ftrs, 1)\n",
    "        # self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.Tensor(x)\n",
    "        \n",
    "        out = F.relu(self.rn(x))\n",
    "#         out = self.rn18(x)\n",
    "#         out = self.rn18.fc(x)\n",
    "#         out1 = self.classifier1(out)\n",
    "#         out2 = self.classifier2(out)\n",
    "        #  out = self.dropout(out)\n",
    "        # out = self.fc3(out)\n",
    "        \n",
    "        #race\n",
    "        out1 = self.rn.fc1(out)\n",
    "        #gender\n",
    "        out2 = self.rn.fc2(out)\n",
    "#         out2 = torch.sigmoid(out2)\n",
    "#         out2 = out2.view(-1)\n",
    "        #age\n",
    "        out3 = self.rn.fc3(out)\n",
    "        out3 = torch.sigmoid(out3)\n",
    "        out3 = out3.view(-1)\n",
    "\n",
    "        return out1, out2, out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-11T06:14:50.192806Z",
     "iopub.status.busy": "2022-01-11T06:14:50.192453Z",
     "iopub.status.idle": "2022-01-11T06:14:50.202098Z",
     "shell.execute_reply": "2022-01-11T06:14:50.200857Z",
     "shell.execute_reply.started": "2022-01-11T06:14:50.192759Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MAEtoMTL(nn.Module):\n",
    "    def __init__(self, image_channel, image_size, patch_size, enc_dim, dec_dim, encoder, decoder, mask_ratio=0.75):\n",
    "        super(MAEtoMTL, self).__init__()\n",
    "\n",
    "        self.mae = MAE(image_channel, image_size, patch_size, enc_dim, dec_dim, encoder, decoder, mask_ratio=0.75)\n",
    "        self.mtl = MTL()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        output =  self.mae(x)\n",
    "        final_outputs = self.mtl(output)\n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-11T06:14:52.763949Z",
     "iopub.status.busy": "2022-01-11T06:14:52.763622Z",
     "iopub.status.idle": "2022-01-11T06:14:56.112929Z",
     "shell.execute_reply": "2022-01-11T06:14:56.112176Z",
     "shell.execute_reply.started": "2022-01-11T06:14:52.763906Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAEtoMTL(\n",
      "  (mae): MAE(\n",
      "    (shuffler): PatchShuffler()\n",
      "    (in_proj): Linear(in_features=768, out_features=512, bias=True)\n",
      "    (encoder): Transformer(\n",
      "      (transformer): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "          (1): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "          (2): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "          (3): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "          (4): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "          (5): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "          (6): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "          (7): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "          (8): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "          (9): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "          (10): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "          (11): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (mid_proj): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (decoder): Transformer(\n",
      "      (transformer): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "          (1): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "          (2): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "          (3): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "          (4): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "          (5): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "          (6): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "          (7): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "          (8): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "          (9): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "          (10): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "          (11): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0, inplace=False)\n",
      "            (dropout2): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (out_proj): Linear(in_features=256, out_features=768, bias=True)\n",
      "  )\n",
      "  (mtl): MTL(\n",
      "    (rn): ResNet(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (4): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (5): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (fc): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (fc1): Linear(in_features=512, out_features=5, bias=True)\n",
      "      (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
      "      (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "model = MAEtoMTL(\n",
    "        image_size=196,\n",
    "        image_channel=3,\n",
    "        patch_size=16,\n",
    "        enc_dim=512,\n",
    "        dec_dim=256,\n",
    "        encoder=dict(\n",
    "            num_layers=12,\n",
    "            norm=None,\n",
    "            nhead=8,\n",
    "            dim_feedforward=2048,\n",
    "            dropout=0,\n",
    "            activation='relu'),\n",
    "        decoder=dict(\n",
    "            num_layers=12,\n",
    "            norm=None,\n",
    "                nhead=4,\n",
    "                dim_feedforward=1024,\n",
    "                dropout=0,\n",
    "                activation='relu'),\n",
    "        mask_ratio=0.75)\n",
    "\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtOkPO6Ga0Fw"
   },
   "source": [
    "### Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-11T06:14:56.126198Z",
     "iopub.status.busy": "2022-01-11T06:14:56.126029Z",
     "iopub.status.idle": "2022-01-11T06:14:56.132253Z",
     "shell.execute_reply": "2022-01-11T06:14:56.131764Z",
     "shell.execute_reply.started": "2022-01-11T06:14:56.126179Z"
    },
    "id": "IoePct00RIFY"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "# criterion2 = nn.BCELoss()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "criterion3 = nn.MSELoss()\n",
    "# criterion2 = nn.MultiLabelSoftMarginLoss()\n",
    "# criterion2 = nn.CrossEntropyLoss() \n",
    "# optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "# optimizer = optim.SGD(model.parameters(), lr = 0.0001)\n",
    "optimizer = optim.AdamW(model.parameters(), lr = 0.0001)\n",
    "\n",
    "criterion1 = criterion1.to(device)\n",
    "criterion2 = criterion2.to(device)\n",
    "criterion3 = criterion3.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-11T06:15:04.248536Z",
     "iopub.status.busy": "2022-01-11T06:15:04.248109Z",
     "iopub.status.idle": "2022-01-11T06:15:04.253963Z",
     "shell.execute_reply": "2022-01-11T06:15:04.252859Z",
     "shell.execute_reply.started": "2022-01-11T06:15:04.248482Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-11T06:15:20.712544Z",
     "iopub.status.busy": "2022-01-11T06:15:20.712080Z",
     "iopub.status.idle": "2022-01-11T06:15:20.722162Z",
     "shell.execute_reply": "2022-01-11T06:15:20.720927Z",
     "shell.execute_reply.started": "2022-01-11T06:15:20.712489Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lr_lambda(max_epochs):\n",
    "        if epoch < args.epochs_warmup:\n",
    "            p = epoch / args.epochs_warmup\n",
    "            lr = args.warmup_from + p * (args.warmup_to - args.warmup_from)\n",
    "        else:\n",
    "            eta_min = args.lr * (args.lr_decay_rate ** 3)\n",
    "            lr = eta_min + (args.lr - eta_min) * (1 + math.cos(math.pi * epoch / args.epochs)) / 2\n",
    "        return lr\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zle9KuFcbwMP"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZFxE7Y9iLfl"
   },
   "source": [
    "#### Train functionss and total accuracy.\n",
    "\n",
    "Hint: [torch.max()](https://pytorch.org/docs/stable/generated/torch.max.html#torch-max) or [torch.argmax()](https://pytorch.org/docs/stable/generated/torch.argmax.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-11T06:15:24.059810Z",
     "iopub.status.busy": "2022-01-11T06:15:24.059388Z",
     "iopub.status.idle": "2022-01-11T06:15:24.376524Z",
     "shell.execute_reply": "2022-01-11T06:15:24.375706Z",
     "shell.execute_reply.started": "2022-01-11T06:15:24.059757Z"
    },
    "id": "VM93brDshO6E"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train(input_data, model, criterion1, criterion2, criterion3, optimizer):\n",
    "    '''\n",
    "    Argement:\n",
    "    input_data -- iterable data, typr torch.utils.data.Dataloader is prefer\n",
    "    model -- nn.Module, model contain forward to predict output\n",
    "    criterion -- loss function, used to evaluate goodness of model\n",
    "    optimizer -- optmizer function, method for weight updating\n",
    "    '''\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "#     f1 = 0.0\n",
    "    total_count = 0\n",
    "    acc_count_1 = 0\n",
    "    acc_count_2 = 0\n",
    "    acc_count_3 = 0\n",
    "    for i in input_data:\n",
    "        images = i[0].to(device)\n",
    "        cate = i[1].to(device)\n",
    "        attr = i[2].to(device)\n",
    "        age = i[3].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs1, outputs2, outputs3 = model(images)\n",
    "\n",
    "        loss1 = criterion1(outputs1, cate)\n",
    "        loss2 = criterion2(outputs2, attr)\n",
    "        loss3 = criterion3(outputs3, age.float())\n",
    "        loss = loss1 + loss2 + loss3\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        _, predicted_ca = torch.max(outputs1.data, 1)\n",
    "        _, predicted_at = torch.max(outputs2.data, 1)\n",
    "        total_count += cate.size(0)\n",
    "        #binary using acc\n",
    "        acc_count_1 += (predicted_ca == cate).sum().item()\n",
    "\n",
    "        #cate using acc\n",
    "#         _, predicted_at = torch.max(outputs2.data, 1)\n",
    "#         acc_count_2 += (predicted_at == attr).sum().item()\n",
    "        acc_count_2 += (predicted_at == attr).sum().item()\n",
    "        \n",
    "        acc_count_3 += ((outputs3 > 0.5) == age).sum().item()\n",
    "        task1_loss = loss1\n",
    "        task2_loss = loss2\n",
    "        task3_loss = loss3\n",
    "\n",
    "        \n",
    "#         predicted_at = (torch.sigmoid(outputs2) > 0.5).int()\n",
    "        #attr using F1\n",
    "\n",
    "#         f1+= f1_score(attr.int().to('cpu').numpy(),predicted_at.to('cpu').numpy(),average='samples')\n",
    "        \n",
    "\n",
    "    # Compute this epoch accuracy and loss\n",
    "    acc_1 = acc_count_1 / total_count\n",
    "    acc_2 = acc_count_2 / total_count\n",
    "    acc_3 = acc_count_3 / total_count\n",
    "#     F1 = f1 / len(loss_list) \n",
    "    loss = sum(loss_list) / len(loss_list)\n",
    "    return acc_1, acc_2, acc_3, task1_loss, task2_loss, task3_loss, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmDy1GTq_H2a"
   },
   "source": [
    "#### Validate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-11T06:15:26.803905Z",
     "iopub.status.busy": "2022-01-11T06:15:26.803495Z",
     "iopub.status.idle": "2022-01-11T06:15:26.820034Z",
     "shell.execute_reply": "2022-01-11T06:15:26.819242Z",
     "shell.execute_reply.started": "2022-01-11T06:15:26.803852Z"
    },
    "id": "USzbBgGEoTRu"
   },
   "outputs": [],
   "source": [
    "def val(input_data, model, criterion1, criterion2, criterion3):\n",
    "    model.eval()\n",
    "    \n",
    "    loss_list = []\n",
    "    total_count = 0\n",
    "    acc_count_1 = 0\n",
    "    acc_count_2 = 0\n",
    "    acc_count_3 = 0\n",
    "#     f1 = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i in input_data:\n",
    "            images = i[0].to(device)\n",
    "            cate = i[1].to(device)\n",
    "            attr = i[2].to(device)\n",
    "            age = i[3].to(device)\n",
    "\n",
    "            outputs1, outputs2, outputs3 = model(images)\n",
    "            loss1 = criterion1(outputs1, cate)\n",
    "            loss2 = criterion2(outputs2, attr)\n",
    "            loss3 = criterion3(outputs3, age.float())\n",
    "            loss = loss1 + loss2 + loss3\n",
    "            \n",
    "            loss_list.append(loss.item())\n",
    "            \n",
    "            _, predicted_cate = torch.max(outputs1.data, 1)\n",
    "            _, predicted_at = torch.max(outputs2.data, 1)\n",
    "            total_count += cate.size(0)\n",
    "            acc_count_1 += (predicted_cate == cate).sum().item()\n",
    "\n",
    "            #cate using acc\n",
    "#             _, predicted_at = torch.max(outputs2.data, 1)\n",
    "#             acc_count_2 += (predicted_at == attr).sum().item()\n",
    "            acc_count_2 += (predicted_at == attr).sum().item()\n",
    "            task1_loss = loss1\n",
    "            task2_loss = loss2\n",
    "            task3_loss = loss3\n",
    "            acc_count_3 += ((outputs3 > 0.5) == age).sum().item()\n",
    "#             predicted_attr = (torch.sigmoid(outputs2) > 0.5).int()\n",
    "#             f1+= f1_score(attr.int().to('cpu').numpy(),predicted_attr.to('cpu').numpy(),average='samples')\n",
    "\n",
    "            \n",
    "    acc_1 = acc_count_1 / total_count\n",
    "    acc_2 = acc_count_2 / total_count\n",
    "    acc_3 = acc_count_3 / total_count\n",
    "#     F1 = f1 / len(loss_list)\n",
    "    loss = sum(loss_list) / len(loss_list)\n",
    "    return acc_1, acc_2, acc_3, task1_loss, task2_loss, task3_loss, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knXu74jCiuxP"
   },
   "source": [
    "#### Training in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "rcVulKkFJRtI",
    "outputId": "61d67635-e91f-4ece-e307-50bb038483aa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# max_epochs = 10\n",
    "log_interval = 1 # print acc and loss in per log_interval time\n",
    "\n",
    "train_acc1_list = []\n",
    "train_acc2_list = []\n",
    "train_acc3_list = []\n",
    "train_loss1_list = []\n",
    "train_loss2_list = []\n",
    "train_loss3_list = []\n",
    "# train_f1_list = []\n",
    "train_loss_list = []\n",
    "val_acc1_list = []\n",
    "val_acc2_list = []\n",
    "val_acc3_list = []\n",
    "val_loss1_list = []\n",
    "val_loss2_list = []\n",
    "val_loss3_list = []\n",
    "# val_f1_list = []\n",
    "val_loss_list = []      \n",
    "        \n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    train_acc1, train_acc2, train_acc3, train_loss1, train_loss2, train_loss3, train_loss = train(train_loader, model, criterion1, criterion2, criterion3, optimizer)\n",
    "    val_acc1, val_acc2, val_acc3, val_loss1, val_loss2, val_loss3, val_loss = val(val_loader, model, criterion1, criterion2, criterion3)\n",
    "    train_acc1_list.append(train_acc1)\n",
    "    train_acc2_list.append(train_acc2)\n",
    "    train_acc3_list.append(train_acc3)\n",
    "    train_loss1_list.append(train_loss1)\n",
    "    train_loss2_list.append(train_loss2)\n",
    "    train_loss3_list.append(train_loss3)\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    val_acc1_list.append(val_acc1)\n",
    "    val_acc2_list.append(val_acc2)\n",
    "    val_acc3_list.append(val_acc3)\n",
    "    val_loss1_list.append(val_loss1)\n",
    "    val_loss2_list.append(val_loss2)\n",
    "    val_loss3_list.append(val_loss3)\n",
    "    val_loss_list.append(val_loss)\n",
    "    if epoch % log_interval == 0:          \n",
    "        print('=' * 40, 'Epoch', epoch, '=' * 40)     \n",
    "        print('task1 Train Acc: {:.6f} task2 Train Acc: {:.6f} task3 Train Acc: {:.6f} task1 Train Loss: {:.6f} task2 Train Loss: {:.6f} task3 Train Loss: {:.6f}  Train Loss: {:.6f}'.format(train_acc1, train_acc2, train_acc3, train_loss1, train_loss2, train_loss3, train_loss)) \n",
    "        print('task1   Val Acc: {:.6f} task2   Val Acc: {:.6f} task3   Val Acc: {:.6f} task1   Val Loss: {:.6f} task2   Val Loss: {:.6f} task3   Val Loss: {:.6f}    Val Loss: {:.6f}'.format(val_acc1, val_acc2, val_acc3, val_loss1, val_loss2, val_loss3, val_loss))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdBlib2wzYU7"
   },
   "outputs": [],
   "source": [
    "# save your well-trained state_dict of model          \n",
    "torch.save(model.state_dict(), 'MAEtoMTL.pt')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5pW9zcKAN-2"
   },
   "source": [
    "#### Visualize accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "id": "4ifzgfp7iq2m",
    "outputId": "e7ce1d4c-0f6e-4fd9-eaeb-760aab8fbc7c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(range(len(train_loss_list)), train_loss_list)\n",
    "plt.plot(range(len(val_loss_list)), val_loss_list, c='r')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(range(len(train_acc1_list)), train_acc1_list)\n",
    "plt.plot(range(len(val_acc1_list)), val_acc1_list, c='r')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.title('Acc')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(range(len(train_acc2_list)), train_acc2_list)\n",
    "plt.plot(range(len(val_acc2_list)), val_acc2_list, c='r')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.title('Acc')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(range(len(train_acc3_list)), train_acc3_list)\n",
    "plt.plot(range(len(val_acc3_list)), val_acc3_list, c='r')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.title('Acc')\n",
    "plt.show()\n",
    "\n",
    "train_loss1_list = np.array(torch.tensor(train_loss1_list, device='cpu'))\n",
    "val_loss1_list = np.array(torch.tensor(val_loss1_list, device='cpu'))\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(range(len(train_loss1_list)), train_loss1_list)\n",
    "plt.plot(range(len(val_loss1_list)), val_loss1_list, c='r')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.title('Task1 Loss')\n",
    "plt.show()\n",
    "\n",
    "train_loss2_list = np.array(torch.tensor(train_loss2_list, device='cpu'))\n",
    "val_loss2_list = np.array(torch.tensor(val_loss2_list, device='cpu'))\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(range(len(train_loss2_list)), train_loss2_list)\n",
    "plt.plot(range(len(val_loss2_list)), val_loss2_list, c='r')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.title('Task2 Loss')\n",
    "plt.show()\n",
    "\n",
    "train_loss3_list = np.array(torch.tensor(train_loss3_list, device='cpu'))\n",
    "val_loss3_list = np.array(torch.tensor(val_loss3_list, device='cpu'))\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(range(len(train_loss3_list)), train_loss3_list)\n",
    "plt.plot(range(len(val_loss3_list)), val_loss3_list, c='r')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.title('Task3 Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMYSgO5viYOk"
   },
   "source": [
    "### Predict Result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHc4hsPMzYU8"
   },
   "outputs": [],
   "source": [
    "# # if you wanna load previous best model\n",
    "# ckpt = torch.load('MTL.pt')\n",
    "# model.load_state_dict(ckpt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SiyK25P6KXrn"
   },
   "outputs": [],
   "source": [
    "def predict(input_data, model):\n",
    "    model.eval()\n",
    "    cate_list = []\n",
    "    attr_list = []\n",
    "    age_list = []\n",
    "    with torch.no_grad():\n",
    "        for images in input_data:\n",
    "            max_age = 116\n",
    "            images = images.to(device)\n",
    "            output1, output2, output3 = model(images)\n",
    "            _, predicted_cate = torch.max(output1.data, 1)\n",
    "            _, predicted_attr = torch.max(output2.data, 1)\n",
    "#             predicted_attr = torch.sigmoid(output2) > 0.5\n",
    "            predicted_age = output3 * max_age\n",
    "            cate_list.extend(predicted_cate.to('cpu').numpy().tolist())\n",
    "            attr_list.extend(predicted_attr.to('cpu').numpy().tolist())\n",
    "            age_list.extend(predicted_age.to('cpu').numpy().tolist())\n",
    "            \n",
    "    return cate_list, attr_list, age_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0I0LN7HwpnsX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder = 'UTKFace_csv'\n",
    "cate_csv, attr_csv, age_csv = predict(test_loader, model)\n",
    "with open('UTKFace_csv/result_race.csv', 'w', newline='') as csvFile:\n",
    "    writer = csv.DictWriter(csvFile, fieldnames=['file_path', 'race_label'])\n",
    "    writer.writeheader()\n",
    "    idx = 0\n",
    "    for result in cate_csv:\n",
    "        file_path = dataset_test.data_list[idx].replace(data_folder + '/', '')\n",
    "        file_path = data_folder + '/' + file_path\n",
    "        writer.writerow({'file_path':file_path, 'race_label':result})\n",
    "        idx += 1\n",
    "\n",
    "with open('UTKFace_csv/result_gender.csv', 'w', newline='') as csvFile:\n",
    "    writer = csv.DictWriter(csvFile, fieldnames=['file_path', 'gender_label'])\n",
    "    writer.writeheader()\n",
    "    idx = 0\n",
    "    for result in attr_csv:\n",
    "        file_path = dataset_test.data_list[idx].replace(data_folder + '/', '')\n",
    "        file_path = data_folder + '/' + file_path\n",
    "        writer.writerow({'file_path':file_path, 'gender_label':result})\n",
    "        idx += 1\n",
    "        \n",
    "\n",
    "with open('UTKFace_csv/result_age.csv', 'w', newline='') as csvFile:\n",
    "    writer = csv.DictWriter(csvFile, fieldnames=['file_path', 'age_label'])\n",
    "    writer.writeheader()\n",
    "    idx = 0\n",
    "    for result in age_csv:\n",
    "        file_path = dataset_test.data_list[idx].replace(data_folder + '/', '')\n",
    "        file_path = data_folder + '/' + file_path\n",
    "        writer.writerow({'file_path':file_path, 'age_label':result})\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open(\"UTKFace/4_1_1_20170112210910341.jpg.chip.jpg\")\n",
    "(w, h) = img.size\n",
    "print('w=%d, h=%d' % (w, h))\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "result1 = pd.read_csv('result_age.csv')\n",
    "origin1 = pd.read_csv('test.csv')\n",
    "\n",
    "total = pd.concat([result1, origin1], axis=1, ignore_index=True)\n",
    "# print(total.columns)\n",
    "# total[1]\n",
    "total = total[(total[1] - total[3]) > 10]\n",
    "\n",
    "# total = total.to_frame()\n",
    "\n",
    "\n",
    "for i, j in zip(total[0][:11], total[1][:11]):\n",
    "        print(i)\n",
    "        print(j)\n",
    "        img = Image.open(i[12:])\n",
    "        (w, h) = img.size\n",
    "        \n",
    "        img.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = Image.open(\"UTKFace/25_0_0_20170117140540912.jpg.chip.jpg\")\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_trans = transforms_test(img)\n",
    "type(img_trans)\n",
    "img_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model =  MTL_Model()\n",
    "ckpt = torch.load('MTL.pt')\n",
    "model.load_state_dict(ckpt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "race, gender, age = model(img_trans.unsqueeze(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "race # white, balck, asian, india, others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "race_p = torch.softmax(race, dim=1)\n",
    "race_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "age.item() * 116"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "assignment4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
